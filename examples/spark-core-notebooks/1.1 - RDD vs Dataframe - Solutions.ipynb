{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD vs DataFrame: Performance & API Comparison\n",
    "\n",
    "This notebook demonstrates the fundamental differences between RDDs and DataFrames using word count on the **Complete Works of Shakespeare**.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Aspect | RDD | DataFrame |\n",
    "|--------|-----|------------|\n",
    "| **Paradigm** | Imperative (HOW) | Declarative (WHAT) |\n",
    "| **Optimization** | None - executes as written | Catalyst optimizer |\n",
    "| **Execution** | Java object serialization | Tungsten + code generation |\n",
    "| **Type Safety** | Compile-time (Scala) | Runtime (schema) |\n",
    "| **Best For** | Low-level control, unstructured data | Structured/semi-structured data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode, split, lower, col, regexp_replace, trim, length\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare text\n",
    "FILEPATH = \"shakespeare.txt\"\n",
    "URL = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    "\n",
    "if not Path(FILEPATH).exists():\n",
    "    print(f\"Downloading from {URL}...\")\n",
    "    urllib.request.urlretrieve(URL, FILEPATH)\n",
    "\n",
    "file_size = Path(FILEPATH).stat().st_size\n",
    "print(f\"File size: {file_size / 1024:.1f} KB ({file_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD_vs_DataFrame_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RDD Implementation\n",
    "\n",
    "With RDDs, you specify **HOW** to process data step by step. Spark executes exactly what you write with no optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_rdd(filepath: str):\n",
    "    \"\"\"\n",
    "    Classic RDD word count - imperative style.\n",
    "    \n",
    "    Step by step:\n",
    "    1. textFile()     -> Read lines from file\n",
    "    2. flatMap()      -> Split lines into words (one row per word)\n",
    "    3. map()          -> Normalize each word\n",
    "    4. filter()       -> Remove empty strings\n",
    "    5. map()          -> Create (word, 1) pairs\n",
    "    6. reduceByKey()  -> Sum counts per word\n",
    "    7. sortBy()       -> Order by count\n",
    "    \"\"\"\n",
    "    return (\n",
    "        sc.textFile(filepath)\n",
    "        .flatMap(lambda line: line.split())\n",
    "        .map(lambda word: ''.join(c for c in word.lower() if c.isalpha()))\n",
    "        .filter(lambda word: len(word) > 0)\n",
    "        .map(lambda word: (word, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and time\n",
    "start = time.perf_counter()\n",
    "rdd_result = word_count_rdd(FILEPATH).collect()\n",
    "rdd_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"RDD execution time: {rdd_time:.3f}s\")\n",
    "print(f\"\\nTop 10 words:\")\n",
    "for word, count in rdd_result[:10]:\n",
    "    print(f\"  {word:15} {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DataFrame Implementation\n",
    "\n",
    "With DataFrames, you specify **WHAT** you want. The Catalyst optimizer decides **HOW** to execute efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_dataframe(filepath: str):\n",
    "    \"\"\"\n",
    "    DataFrame word count - declarative style.\n",
    "    \n",
    "    Benefits:\n",
    "    - Catalyst optimizer rewrites query for efficiency\n",
    "    - Tungsten engine: off-heap memory, code generation\n",
    "    - Automatic predicate pushdown, column pruning\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.text(filepath)\n",
    "        # Split into words and explode to rows\n",
    "        .select(explode(split(col(\"value\"), r\"\\s+\")).alias(\"word\"))\n",
    "        # Normalize: lowercase, keep only letters\n",
    "        .select(regexp_replace(lower(col(\"word\")), r\"[^a-z]\", \"\").alias(\"word\"))\n",
    "        # Filter empty\n",
    "        .filter(length(col(\"word\")) > 0)\n",
    "        # Group and count\n",
    "        .groupBy(\"word\").count()\n",
    "        # Sort\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and time\n",
    "start = time.perf_counter()\n",
    "df_result = word_count_dataframe(FILEPATH).collect()\n",
    "df_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"DataFrame execution time: {df_time:.3f}s\")\n",
    "print(f\"\\nTop 10 words:\")\n",
    "for row in df_result[:10]:\n",
    "    print(f\"  {row['word']:15} {row['count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ” The Key Insight: Query Execution Plans\n",
    "\n",
    "This is where the real difference becomes visible. The DataFrame's `explain()` shows how Catalyst transforms your query through multiple stages of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query (lazy - not executed yet)\n",
    "df_query = word_count_dataframe(FILEPATH)\n",
    "\n",
    "# Show the full execution plan\n",
    "print(\"=\" * 70)\n",
    "print(\"DATAFRAME EXECUTION PLAN\")\n",
    "print(\"=\" * 70)\n",
    "df_query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Plan\n",
    "\n",
    "The plan shows several optimization stages:\n",
    "\n",
    "1. **Parsed Logical Plan** - What you wrote\n",
    "2. **Analyzed Logical Plan** - Resolved references and types\n",
    "3. **Optimized Logical Plan** - After Catalyst optimization\n",
    "4. **Physical Plan** - Actual execution strategy\n",
    "\n",
    "Key optimizations you'll see:\n",
    "- **WholeStageCodegen** - Compiles query stages to optimized Java bytecode\n",
    "- **HashAggregate** - Efficient partial aggregation before shuffle\n",
    "- **Exchange** - Shuffle operation (note: only where necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs have no equivalent - you can only see the DAG lineage\n",
    "rdd_query = word_count_rdd(FILEPATH)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RDD DEBUG STRING (Lineage only - no optimization info)\")\n",
    "print(\"=\" * 70)\n",
    "print(rdd_query.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarking: Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(name: str, func, n_runs: int = 5, warmup: int = 1):\n",
    "    \"\"\"Run benchmark with warmup.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for i in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"times\": times,\n",
    "        \"mean\": np.mean(times),\n",
    "        \"std\": np.std(times),\n",
    "        \"min\": np.min(times),\n",
    "        \"max\": np.max(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 5\n",
    "\n",
    "print(f\"Running {N_RUNS} benchmark iterations (plus warmup)...\\n\")\n",
    "\n",
    "rdd_bench = benchmark(\n",
    "    \"RDD\", \n",
    "    lambda: word_count_rdd(FILEPATH).collect(),\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "print(f\"RDD:       {rdd_bench['mean']:.3f}s Â± {rdd_bench['std']:.3f}s\")\n",
    "\n",
    "df_bench = benchmark(\n",
    "    \"DataFrame\",\n",
    "    lambda: word_count_dataframe(FILEPATH).collect(),\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "print(f\"DataFrame: {df_bench['mean']:.3f}s Â± {df_bench['std']:.3f}s\")\n",
    "\n",
    "speedup = rdd_bench['mean'] / df_bench['mean']\n",
    "print(f\"\\nðŸš€ DataFrame is {speedup:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Performance Comparison ---\n",
    "ax1 = axes[0]\n",
    "names = [\"RDD\", \"DataFrame\"]\n",
    "means = [rdd_bench[\"mean\"], df_bench[\"mean\"]]\n",
    "stds = [rdd_bench[\"std\"], df_bench[\"std\"]]\n",
    "colors = [\"#e74c3c\", \"#3498db\"]\n",
    "\n",
    "bars = ax1.bar(names, means, yerr=stds, capsize=8, color=colors, \n",
    "               edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "ax1.set_ylabel(\"Execution Time (seconds)\", fontsize=12)\n",
    "ax1.set_title(\"Word Count Performance: RDD vs DataFrame\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.05,\n",
    "            f\"{mean:.3f}s\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Add speedup annotation\n",
    "ax1.annotate(f\"{speedup:.1f}x faster\", \n",
    "            xy=(1, df_bench[\"mean\"]), \n",
    "            xytext=(0.5, (rdd_bench[\"mean\"] + df_bench[\"mean\"])/2),\n",
    "            fontsize=14, fontweight=\"bold\", color=\"#27ae60\",\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"#27ae60\", lw=2))\n",
    "\n",
    "# --- Top Words ---\n",
    "ax2 = axes[1]\n",
    "top_words = df_result[:15]\n",
    "words = [row[\"word\"] for row in top_words]\n",
    "counts = [row[\"count\"] for row in top_words]\n",
    "\n",
    "y_pos = np.arange(len(words))\n",
    "ax2.barh(y_pos, counts, color=\"#3498db\", edgecolor=\"black\", linewidth=0.5)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(words, fontsize=11)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel(\"Frequency\", fontsize=12)\n",
    "ax2.set_title(\"Top 15 Words in Shakespeare\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i, count in enumerate(counts):\n",
    "    ax2.text(count + 200, i, f\"{count:,}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"benchmark_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why DataFrames Are Faster\n",
    "\n",
    "### 1. Catalyst Optimizer\n",
    "- **Predicate pushdown**: Filters moved closer to data source\n",
    "- **Column pruning**: Only reads columns actually needed\n",
    "- **Constant folding**: Evaluates constant expressions at compile time\n",
    "- **Join reordering**: Optimizes join order based on statistics\n",
    "\n",
    "### 2. Tungsten Execution Engine\n",
    "- **Off-heap memory**: Avoids JVM garbage collection overhead\n",
    "- **Cache-aware computation**: Optimized for CPU cache hierarchy\n",
    "- **Whole-stage code generation**: Compiles query stages to optimized bytecode\n",
    "\n",
    "### 3. Efficient Data Representation\n",
    "- RDDs: Java objects with full serialization overhead\n",
    "- DataFrames: Binary row format, compact and efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use RDDs\n",
    "\n",
    "RDDs still have valid use cases:\n",
    "\n",
    "1. **Unstructured data** that doesn't fit a columnar schema\n",
    "2. **Low-level transformations** not expressible in DataFrame API\n",
    "3. **Fine-grained partitioning control** for specific performance tuning\n",
    "4. **Legacy code** that predates DataFrame API\n",
    "5. **Type safety in Scala** with compile-time checking\n",
    "\n",
    "For most structured data processing: **prefer DataFrames**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
